{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No elements detected on page 2, skipping this page.\n",
      "/home/jerome/miniforge3/envs/dev/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from py_pdf_parser.loaders import load_file\n",
    "\n",
    "document = load_file(\"Malo_Adler_Thesis.pdf\")\n",
    "text2 = \"\"\n",
    "for element in document.elements:\n",
    "    text2 += element.text()\n",
    "\n",
    "from extractive_summary import EmbeddingModel\n",
    "embedding_model = EmbeddingModel.from_hugging_hub_sentence_transformer(\"dangvantuan/sentence-camembert-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nThe field of natural language processing (NLP) involves various techniques to improve communication between humans and machines through the use of algorithms and models that can understand, interpret, and generate human-like text. Some key areas within NLP include text classification, question answering, summarization, translation, and generation. Machine learning and deep learning techniques are commonly used in NLP to train models on large amounts of data, allowing them to recognize patterns and make predictions or generate new text based on that understanding. Popular models in NLP include BERT, GPT-3, and LLMs (large language models). Evaluation metrics for summarization tasks include ROUGE, chrF, and Mover-Score, while BERTScore is used to evaluate text generation tasks.\\n\\nThe given documents discuss various models and techniques for text summarization, including graph-based methods, deep neural networks, reinforcement learning, training-free methods using genetic algorithms, and Large Language Models (LLMs). The authors highlight different aspects of the summarization process, such as evaluating factuality and hallucination detection, creating prompts for instruction-tuned LLMs, and optimizing memory requirements with weight quantization. They also discuss various evaluation metrics for summarization tasks, including reference-based and source-based metrics. Some documents focus specifically on summarizing complex texts in different languages like Arabic and Chinese or judicial data summarization, while others explore the use of LLMs for summarization tasks and their cost and resources required.\\n\\nThese summaries cover various pre-trained language models and their applications for natural language processing tasks in French, including search, question-answering, and text generation. Some of these models are specifically designed for French language, while others are bilingual or multilingual. The summaries also discuss legal extractive summarization methods and evaluation. Overall, the summaries highlight the advancements and applications of pre-trained language models in natural language processing tasks.'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from summary_chain import summarize_chain_builder\n",
    "\n",
    "custom_chain = summarize_chain_builder(method=\"k-means\", embedding_model=embedding_model)\n",
    "res = custom_chain.invoke(text2)\n",
    "res"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
