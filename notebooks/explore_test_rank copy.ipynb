{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "OPENAI_API_BASE= os.environ.get(\"OPENAI_API_BASE\", \"https://api-ai.numerique-interieur.com/v1\") \n",
    "OPENAI_API_KEY = os.environ[\"OPENAI_API_KEY\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/.cache/pypoetry/virtualenvs/abrege-9TtSrW0h-py3.12/lib/python3.12/site-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import ChatOpenAI`.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.document_loaders import WebBaseLoader\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains.llm import LLMChain\n",
    "\n",
    "llm = ChatOpenAI(api_key=OPENAI_API_KEY, openai_api_base=OPENAI_API_BASE, temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../big_text.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m../big_text.txt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m      3\u001b[0m     text \u001b[38;5;241m=\u001b[39m f\u001b[38;5;241m.\u001b[39mread()\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/abrege-9TtSrW0h-py3.12/lib/python3.12/site-packages/IPython/core/interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    322\u001b[0m     )\n\u001b[0;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../big_text.txt'"
     ]
    }
   ],
   "source": [
    "text = \"\"\n",
    "with open(\"../big_text.txt\", 'r') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jerome/miniforge3/envs/dev/lib/python3.12/site-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "\n",
    "text_splitter = CharacterTextSplitter.from_tiktoken_encoder(chunk_size=1000, chunk_overlap=0)\n",
    "split_docs = text_splitter.split_documents(text)\n",
    "\n",
    "prompt_template = \"\"\"Write a concise summary of the following:\n",
    "{text}\n",
    "CONCISE SUMMARY:\"\"\"\n",
    "prompt = PromptTemplate.from_template(prompt_template)\n",
    "\n",
    "refine_template = (\n",
    "    \"Your job is to produce a final summary\\n\"\n",
    "    \"We have provided an existing summary up to a certain point: {existing_answer}\\n\"\n",
    "    \"We have the opportunity to refine the existing summary\"\n",
    "    \"(only if needed) with some more context below.\\n\"\n",
    "    \"------------\\n\"\n",
    "    \"{text}\\n\"\n",
    "    \"------------\\n\"\n",
    "    \"Given the new context, refine the original summary in French\"\n",
    "    \"If the context isn't useful, return the original summary.\"\n",
    ")\n",
    "refine_prompt = PromptTemplate.from_template(refine_template)\n",
    "chain = load_summarize_chain(\n",
    "    llm=llm,\n",
    "    chain_type=\"refine\",\n",
    "    question_prompt=prompt,\n",
    "    refine_prompt=refine_prompt,\n",
    "    return_intermediate_steps=True,\n",
    "    input_key=\"input_documents\",\n",
    "    output_key=\"output_text\",\n",
    ")\n",
    "result = chain({\"input_documents\": split_docs}, return_only_outputs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Le lendemain matin, Ned Land et le narrateur découvrirent l'île de Tarawa, située à deux milles au large de la côte de Papouasie-Nouvelle-Guinée. Ils trouvèrent des arbres, des animaux terrestres et des porteurs de couteaux. Le capitaine Nemo expliqua qu'il voulait suivre les traces du _Nautilus_ et découvrir l'endroit où il avait échoué.\n",
      "\n",
      "Ils naviguèrent pendant plusieurs jours, passant devant d'autres îles peuplées de Papouas. Le narrateur remarqua que les habitants étaient effrayés par la présence du monstre échoué dans leur baie. Ils se rapprochèrent alors du _Nautilus_ et l'examinaient avec attention.\n",
      "\n",
      "Le capitaine Nemo décida d'abord de s'approcher à portée de tir des habitants pour les observer de plus près. Il fit ensuite une démonstration de son arbalète, en utilisant un couteau comme flèche et en touchant le but avec précision. Les Papouas étaient fascinés par cette démonstration et commencèrent à s'approcher du _Nautilus_.\n",
      "\n",
      "Le capitaine Nemo décida alors de faire une démonstration de son sous-marin, en montrant les différents systèmes qui le rendaient possible. Il fit ensuite une démonstration de sa capacité à plonger et à remonter rapidement en surface. Les habitants étaient impressionnés par ces démonstrations et commencèrent à s'entraîner avec lui.\n",
      "\n",
      "Cependant, un jour, les Papouas revinrent avec des armes et essayèrent de pénétrer dans le _Nautilus_. Le capitaine Nemo fut obligé d'utiliser sa force pour protéger son bateau. Les habitants s'en allèrent, impressionnés par la puissance du capitaine.\n",
      "\n",
      "Le lendemain matin, le _Nautilus_ reprit sa navigation vers l'ouest, en direction de l'Australie. Le capitaine Nemo avait décidé d'y faire une escale pour réparer les dégâts causés par la tempête qui avait fait échouer son bateau sur l'île de Tarawa.\n",
      "\n",
      "Dans le cadre de cette escale, le _Nautilus_ se dirigea vers un port situé à proximité de l'île. Le capitaine Nemo décida d'abord de s'approcher à portée de tir des habitants pour les observer de plus près. Il fit ensuite une démonstration de son arbalète, en utilisant un couteau comme flèche et en touchant le but avec précision. Les Papouas étaient fascinés par cette démonstration et commencèrent à s'approcher du _Nautilus_.\n",
      "\n",
      "Le capitaine Nemo décida alors de faire une démonstration de son sous-marin, en montrant les différents systèmes qui le rendaient possible. Il fit ensuite une démonstration de sa capacité à plonger et à remonter rapidement en surface. Les habitants étaient impressionnés par ces démonstrations et commencèrent à s'entraîner avec lui.\n",
      "\n",
      "Cependant, un jour, les Papouas revinrent avec des armes et essayèrent de pénétrer dans le _Nautilus_. Le capitaine Nemo fut obligé d'utiliser sa force pour protéger son bateau. Les habitants s'en allèrent, impressionnés par la puissance du capitaine.\n",
      "\n",
      "Le lendemain matin, le\n"
     ]
    }
   ],
   "source": [
    "print(result['output_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import MapReduceDocumentsChain, ReduceDocumentsChain\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "\n",
    "\n",
    "# Map\n",
    "map_template = \"\"\"The following is a set of documents\n",
    "{docs}\n",
    "Based on this list of docs, please identify the main themes \n",
    "Helpful Answer:\"\"\"\n",
    "map_prompt = PromptTemplate.from_template(map_template)\n",
    "map_chain = LLMChain(llm=llm, prompt=map_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce\n",
    "reduce_template = \"\"\"The following is set of summaries:\n",
    "{docs}\n",
    "Take these and distill it into a final, consolidated summary of the main themes. \n",
    "Helpful Answer:\"\"\"\n",
    "reduce_prompt = PromptTemplate.from_template(reduce_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.combine_documents.stuff import StuffDocumentsChain\n",
    "# Run chain\n",
    "reduce_chain = LLMChain(llm=llm, prompt=reduce_prompt)\n",
    "\n",
    "# Takes a list of documents, combines them into a single string, and passes this to an LLMChain\n",
    "combine_documents_chain = StuffDocumentsChain(\n",
    "    llm_chain=reduce_chain, document_variable_name=\"docs\"\n",
    ")\n",
    "\n",
    "# Combines and iteratively reduces the mapped documents\n",
    "reduce_documents_chain = ReduceDocumentsChain(\n",
    "    # This is final chain that is called.\n",
    "    combine_documents_chain=combine_documents_chain,\n",
    "    # If documents exceed context for `StuffDocumentsChain`\n",
    "    collapse_documents_chain=combine_documents_chain,\n",
    "    # The maximum number of tokens to group documents into.\n",
    "    token_max=4000,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combining documents by mapping a chain over them, then combining results\n",
    "map_reduce_chain = MapReduceDocumentsChain(\n",
    "    # Map chain\n",
    "    llm_chain=map_chain,\n",
    "    # Reduce chain\n",
    "    reduce_documents_chain=reduce_documents_chain,\n",
    "    # The variable name in the llm_chain to put the documents in\n",
    "    document_variable_name=\"docs\",\n",
    "    # Return the results of the map steps in the output\n",
    "    return_intermediate_steps=False,\n",
    ")\n",
    "\n",
    "text_splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=1000, chunk_overlap=0\n",
    ")\n",
    "split_docs = text_splitter.split_documents(pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_map_reduce = map_reduce_chain.run(split_docs) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "loader = PyPDFLoader(\"../Malo_Adler_Thesis.pdf\")\n",
    "pages = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2791"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if 0:\n",
    "    text = \"\"\n",
    "    for page in pages:\n",
    "        text += page.page_content\n",
    "\n",
    "from pathlib import Path\n",
    "text = Path(\"./../src/camus.txt\").read_text()\n",
    "llm.get_num_tokens(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'HuggingFaceEmbeddings' object has no attribute 'encode'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 12\u001b[0m\n\u001b[1;32m      8\u001b[0m prompt \u001b[38;5;241m=\u001b[39m PromptTemplate\u001b[38;5;241m.\u001b[39mfrom_template(prompt_template)\n\u001b[1;32m     10\u001b[0m llm_chain \u001b[38;5;241m=\u001b[39m LLMChain(llm\u001b[38;5;241m=\u001b[39mllm, prompt\u001b[38;5;241m=\u001b[39mprompt)\n\u001b[0;32m---> 12\u001b[0m text_prompt \u001b[38;5;241m=\u001b[39m \u001b[43mbuild_text_prompt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mHuggingFaceEmbeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mOrdalieTech/Solon-embeddings-base-0.1\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m chain \u001b[38;5;241m=\u001b[39m load_summarize_chain(llm, chain_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstuff\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     16\u001b[0m result \u001b[38;5;241m=\u001b[39m llm_chain\u001b[38;5;241m.\u001b[39minvoke({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m: text_prompt})\n",
      "File \u001b[0;32m/app/notebooks/textrank.py:203\u001b[0m, in \u001b[0;36mbuild_text_prompt\u001b[0;34m(text, size, embedding_model)\u001b[0m\n\u001b[1;32m    201\u001b[0m sentence_result \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 203\u001b[0m     idx_next \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msentence_idx_iterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    204\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m sentence_len \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mlen\u001b[39m(list_sentences[idx_next]) \u001b[38;5;241m>\u001b[39m size:\n\u001b[1;32m    205\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/app/notebooks/textrank.py:138\u001b[0m, in \u001b[0;36mtext_rank_iterator\u001b[0;34m(list_sentences, embedding_model)\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    121\u001b[0m \u001b[38;5;124;03mYield the top sentences of the models, according to the embdeddings delivred\u001b[39;00m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;124;03mby embeddding_model\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;124;03m    index of the best sentences so far\u001b[39;00m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;66;03m# Next build a similarity relation between each pair of sentences\u001b[39;00m\n\u001b[0;32m--> 138\u001b[0m dict_weight \u001b[38;5;241m=\u001b[39m \u001b[43mbuild_weight\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlist_sentences\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding_model\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;66;03m# Build the graph\u001b[39;00m\n\u001b[1;32m    141\u001b[0m graph \u001b[38;5;241m=\u001b[39m build_graph(list_sentences, dict_weight)\n",
      "File \u001b[0;32m/app/notebooks/textrank.py:78\u001b[0m, in \u001b[0;36mbuild_weight\u001b[0;34m(list_sentence, model)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;124;03mcompute similarity between each pair of sentences and put them in a dict\u001b[39;00m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;124;03mmapped with their cosine simalirity\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;124;03m    to their cosine similarity\u001b[39;00m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     77\u001b[0m result \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m---> 78\u001b[0m evaluations \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m(\n\u001b[1;32m     79\u001b[0m     list_sentence,\n\u001b[1;32m     80\u001b[0m     normalize_embeddings\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     81\u001b[0m     convert_to_numpy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     82\u001b[0m     convert_to_tensor\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     83\u001b[0m )\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(list_sentence):\n\u001b[1;32m     86\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mlen\u001b[39m(list_sentence)):\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'HuggingFaceEmbeddings' object has no attribute 'encode'"
     ]
    }
   ],
   "source": [
    "from textrank import build_text_prompt\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "\n",
    "prompt_template = \"\"\"Ecrit un résumé en français du texte suivant en détaillant le plus possible:\n",
    "\"{text}\"\n",
    "RESUME:\"\"\"\n",
    "prompt = PromptTemplate.from_template(prompt_template)\n",
    "\n",
    "llm_chain = LLMChain(llm=llm, prompt=prompt)\n",
    "\n",
    "text_prompt = build_text_prompt(text, 3000, HuggingFaceEmbeddings(model_name=\"OrdalieTech/Solon-embeddings-base-0.1\"))\n",
    "\n",
    "chain = load_summarize_chain(llm, chain_type=\"stuff\")\n",
    "\n",
    "result = llm_chain.invoke({'text': text_prompt})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nThe given text discusses various aspects related to natural language processing (NLP), machine learning, and data analysis using pre-trained language models (LLMs). The first document introduces the BERTScore metric, which uses contextual word embeddings from the BERT model to compare the semantics of two texts by comparing the embeddings of each word in one text to the embeddings of each word in the other text. The second and third documents discuss various metrics for evaluating factuality and summarization tasks using pre-trained LLMs, including SelfCheckGPT-Prompt and SelfCheckGPT-BERT, which aim to provide a more comprehensive evaluation of language models' ability to generate accurate and coherent summaries by generating multiple summaries and checking if the model's output is supported by each summary. The third document evaluates different pre-trained LLMs for abstractive summarization using basic summarization prompts, and it is observed that larger models generally perform better than smaller ones.\\n\\nThe documents in the given list cover various aspects of NLP, machine learning, data analysis, research institutions, ethics, hardware acceleration, reinforcement learning, and human-in-the-loop methods. Some papers also explore the use of language models in various domains, including legal summarization and expert systems.\\n\\nOverall, the main themes in these documents are the evaluation of different summarization algorithms and their performance using various metrics, the importance of selecting appropriate metrics for evaluating summarization tasks, and the need to fine-tune pre-trained LLMs for specific tasks. Additionally, some papers discuss ethical considerations in AI development and deployment, including issues related to bias and fairness in machine learning models.\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_map_reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': \"Le but principal de cette thèse est d'évaluer comment les modèles de langage pré-entraînés peuvent se comporter sur ce type de données spécifiques et comment améliorer ces performances. Une méthode hybride est souvent considérée pour résoudre les contraintes de taille (voir Section 2.4.4), qui consiste à réduire une version longue du texte par des méthodes extraitives, puis à générer un résumé abstrait de cette version réduite. Cette approche présente plusieurs avantages: le résumé produit est plus proche d'un texte écrit à la main; le texte généré est plus facile à comprendre et plus lisible; le résultat peut être potentiellement plus exhaustif.\\n\\nL'algorithme K-Means, appliqué à la summarisation textuelle extraitives, présente un inconvénient majeur: le nombre de phrases souhaitées dans le résumé final doit être spécifié en avance car l'algorithme K-Means nécessite de connaître avanthand le nombre de clusters à calculer. Enfin, la longueur du contexte de 4096 tokens (environ 3000 mots, y compris le texte d'entrée et l'instruction de summarisation) permet de gérer des textes de taille moyenne, et l'utilisation de l'attention Grouped-Query (GQA) offre un bon équilibre entre les performances et le temps de calcul.\\n\\nDans notre cas, cela implique de répéter les étapes suivantes pour un grand nombre d'exemples: générer une prédiction avec la méthode actuelle et du texte source; comparer cette prédiction à la référence et déduire une perte; propager cette perte à travers le réseau pour mettre à jour les poids de la méthode.\\n\\nLes méthodes d'automatique summarisation, en particulier celles utilisant des modèles de langage grand (LLMs), sont très coûteuses en termes de puissance de calcul et de temps, de RAM, VRAM et de données (pour certaines méthodes seulement, voir Section 2.4.6). En tant que première étape, nous traduisons les articles et leurs candidats résumés en utilisant le traducteur automatique de haute qualité Google Translate (pour ce texte public et non sensible, l'utilisation d'outils tels que cela n'est pas problématique en termes de gouvernance des données). Nous adaptons cette étape de recherche pour sélectionner un nombre donné de textes à partir d'un corpus de textes/résumés qui sont proches sémantiquement de l'ANFSI, Datalab 47 / 73KTH – Royal Institute of Technology, à summariser (pour cette recherche sémantique, nous insérons le texte source ainsi que les textes du corpus avec le paquet de transformateurs de phrases [66], et utilisons la similitude cosine pour calculer les textes les plus proches dans le corpus).\\n\\nEn particulier, il semble qu'é\"}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'For this reason, the other main objective of this thesis\\nwill be to evaluate how pre-trained language models can perform on this specific data, and how\\nthese performances can be improved.However, abstractive methods having size constraints (see Section 2.4.4), a hybrid strategy\\nis often considered [5], consisting of first reducing a long text with extractive methods, and then\\ngenerating an abstractive summary of this reduced version.It has several significant advantages:\\n• The produced summaries are closer to a human-written text;\\n• The generated text is more understandable and easier to read;\\n• The result can potentially be more exhaustive.Figure 8: The K-Means algorithm, applied to extractive text summarization\\nThe main disadvantage of this algorithm is that the number of sentences wanted in the final sum-\\nmary must be specified in advance, because the K-Means algorithm requires knowing beforehand\\nthe number of clusters to compute.Finally, the context length of 4096 tokens (around 3000 words, including the in-\\nput text and the summarization instruction) makes it possible to handle medium sized texts, and\\nthe use of Grouped-Query Attention (GQA) provides a good trade-o ffbetween performance and\\ncomputation time.Concretely, in our case, it involves repeating the following steps for a large number\\nof examples:\\n• Generate, with the current model and from the source, a prediction;\\n• Compare this prediction to the reference, and deduce a loss;\\n• Backpropagate this loss through the network to update the model weights.3.1 Resources\\nThe use of certain automatic summarization methods, and in particular that of Large Language\\nModels (LLMs), is very costly in terms of computation power and time, RAM, VRAM, and data\\n(the latter concerning some methods only, see Section 2.4.6).As a first step, we translate the articles and\\ntheir candidate summaries using the high-quality translator Google Translate (for this public,\\nnon-sensitive data, using such tools is not problematic in terms of data governance).We adapt this retrieval step to select\\na given number of texts, from a corpus of text/summary pairs, that are semantically close to the\\nANFSI, Datalab 47 / 73KTH – Royal Institute of Technology\\ntext to summarize (for this semantic search, we embed the source text, as well as the corpus texts,\\nwith the sentence-transformers package [66], and use cosine similarity to compute the closest\\ntexts in the corpus).In particular, it seems that generating several samples and compar-\\ning the BERTScore between these samples and the prediction (done in the ‘Multi check’ metric,\\nwhich is our implementation of SelfCheckGPT-BERT) is clearly less e fficient than simply using\\nthe LLM itself as double-checker.These results are quite classi-\\ncal: first, the model impressively gets better; then, it reaches a performance peak (around 12-14\\nepochs); and finally, its performance slightly decreases, probably because it starts overfitting the\\ntraining data.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_prompt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
